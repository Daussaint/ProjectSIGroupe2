{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion detection using CNN \n",
    "## A) Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import cv2\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Link for loading the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1) For Romain :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       emotion                                             pixels        Usage\n",
      "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
      "1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
      "2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
      "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
      "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n",
      "...        ...                                                ...          ...\n",
      "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
      "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
      "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
      "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
      "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
      "\n",
      "[35887 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "emotion_data = pd.read_csv('/Users/romai/Documents/Ecole/Ingé - M1_auto/Q2/Système intelligent/fer2013.csv')\n",
    "print(emotion_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2) For Lucas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       emotion                                             pixels        Usage\n",
      "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
      "1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
      "2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
      "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
      "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n",
      "...        ...                                                ...          ...\n",
      "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
      "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
      "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
      "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
      "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
      "\n",
      "[35887 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "emotion_data = pd.read_csv('/Users/Lucas/Documents/Ingenieur_Industriel/Master_1/Q2/Systèmes_intelligents/fer2013.csv')\n",
    "print(emotion_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['emotion', 'pixels', 'Usage'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35887 entries, 0 to 35886\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   emotion  35887 non-null  int64 \n",
      " 1   pixels   35887 non-null  object\n",
      " 2   Usage    35887 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 841.2+ KB\n"
     ]
    }
   ],
   "source": [
    "emotion_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Formation of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Premièrement, création sous forme de List\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Pour chaque ligne de la base de donnée, si il s'agit d'un \"Training\" les données vont être attribuées\n",
    "# à X_train et l'émotion correspondante à y_train\n",
    "# Si il s'agit d'un \"Public test\", les données vont être attribuées à X_test et l'émotion correspondante\n",
    "# à y_test\n",
    "for index, row in emotion_data.iterrows():\n",
    "    k = row['pixels'].split(\" \")\n",
    "    if row['Usage'] == 'Training':\n",
    "        X_train.append(np.array(k,'float32'))\n",
    "        y_train.append(row['emotion'])\n",
    "    elif row['Usage'] == 'PublicTest':\n",
    "        X_test.append(np.array(k,'float32'))\n",
    "        y_test.append(row['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation du list en array et du type des données en float32\n",
    "X_train = np.array(X_train,'float32')\n",
    "y_train = np.array(y_train,'float32')\n",
    "X_test = np.array(X_test,'float32')\n",
    "y_test = np.array(y_test,'float32')\n",
    "\n",
    "# Convertion du vecteur de classe (entiers) en une matrice de classe binaire.\n",
    "y_train= to_categorical(y_train, num_classes=7)\n",
    "y_test = to_categorical(y_test, num_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Normalization of data between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train :  (28709, 48, 48, 1)\n",
      "y_train :  (28709, 7)\n",
      "X_test  :  (3589, 48, 48, 1)\n",
      "y_test  :  (3589, 7)\n"
     ]
    }
   ],
   "source": [
    "# Transformation des éléments des matrices en un nombre entre 0 et 1 pour faciliter l'apprentissage\n",
    "X_train -= np.mean(X_train, axis=0)\n",
    "X_train /= np.std(X_train, axis=0)\n",
    "\n",
    "X_test -= np.mean(X_test, axis=0)\n",
    "X_test /= np.std(X_test, axis=0)\n",
    "\n",
    "# Remodulation de X_train et X_test en image de 48x48 pixels\n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
    "\n",
    "# Visualisation de la forme des matrices\n",
    "print('X_train : ',X_train.shape)\n",
    "print('y_train : ',y_train.shape)\n",
    "print('X_test  : ',X_test.shape)\n",
    "print('y_test  : ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1) Designing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 20, 20, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 18, 18, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 5, 5, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 1,914,951\n",
      "Trainable params: 1,914,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creation d'un model lineaire de piles de couches\n",
    "model = Sequential()\n",
    "\n",
    "# 1nst convolution layer\n",
    "model.add(Convolution2D(64,kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1:])))\n",
    "model.add(Convolution2D(64,kernel_size=(3,3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 2nd convolution layer\n",
    "model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 3nd convolution layer\n",
    "model.add(Convolution2D(128, (3,3), activation='relu'))\n",
    "model.add(Convolution2D(128, (3,3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "# Passage en matrice à 1D\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Probabilité des classes\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# Affichage du sommaire du model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2) Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des pertes et métriques du model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "449/449 [==============================] - 355s 788ms/step - loss: 1.8541 - accuracy: 0.2462 - val_loss: 1.8223 - val_accuracy: 0.2494\n",
      "Epoch 2/50\n",
      "449/449 [==============================] - 385s 858ms/step - loss: 1.8001 - accuracy: 0.2558 - val_loss: 1.8075 - val_accuracy: 0.2494\n",
      "Epoch 3/50\n",
      "449/449 [==============================] - 368s 820ms/step - loss: 1.7893 - accuracy: 0.2532 - val_loss: 1.7999 - val_accuracy: 0.2497\n",
      "Epoch 4/50\n",
      "449/449 [==============================] - 365s 814ms/step - loss: 1.7721 - accuracy: 0.2629 - val_loss: 1.7844 - val_accuracy: 0.2505\n",
      "Epoch 5/50\n",
      "449/449 [==============================] - 379s 844ms/step - loss: 1.7647 - accuracy: 0.2666 - val_loss: 1.7717 - val_accuracy: 0.2575\n",
      "Epoch 6/50\n",
      "449/449 [==============================] - 363s 808ms/step - loss: 1.7538 - accuracy: 0.2784 - val_loss: 1.7453 - val_accuracy: 0.2984\n",
      "Epoch 7/50\n",
      "449/449 [==============================] - 382s 850ms/step - loss: 1.7281 - accuracy: 0.2989 - val_loss: 1.7312 - val_accuracy: 0.2995\n",
      "Epoch 8/50\n",
      "449/449 [==============================] - 371s 827ms/step - loss: 1.7012 - accuracy: 0.3175 - val_loss: 1.6741 - val_accuracy: 0.3254\n",
      "Epoch 9/50\n",
      "449/449 [==============================] - 387s 862ms/step - loss: 1.6782 - accuracy: 0.3256 - val_loss: 1.6683 - val_accuracy: 0.3282\n",
      "Epoch 10/50\n",
      "449/449 [==============================] - 383s 853ms/step - loss: 1.6600 - accuracy: 0.3409 - val_loss: 1.6373 - val_accuracy: 0.3452\n",
      "Epoch 11/50\n",
      "449/449 [==============================] - 384s 856ms/step - loss: 1.6380 - accuracy: 0.3535 - val_loss: 1.6409 - val_accuracy: 0.3383\n",
      "Epoch 12/50\n",
      "449/449 [==============================] - 421s 937ms/step - loss: 1.6159 - accuracy: 0.3636 - val_loss: 1.6328 - val_accuracy: 0.3385\n",
      "Epoch 13/50\n",
      "449/449 [==============================] - 418s 932ms/step - loss: 1.6063 - accuracy: 0.3694 - val_loss: 1.5655 - val_accuracy: 0.3831\n",
      "Epoch 14/50\n",
      "449/449 [==============================] - 423s 942ms/step - loss: 1.5875 - accuracy: 0.3803 - val_loss: 1.5562 - val_accuracy: 0.3842\n",
      "Epoch 15/50\n",
      "449/449 [==============================] - 422s 940ms/step - loss: 1.5683 - accuracy: 0.3876 - val_loss: 1.5370 - val_accuracy: 0.4007\n",
      "Epoch 16/50\n",
      "449/449 [==============================] - 422s 940ms/step - loss: 1.5545 - accuracy: 0.3948 - val_loss: 1.5163 - val_accuracy: 0.4163\n",
      "Epoch 17/50\n",
      "449/449 [==============================] - 423s 942ms/step - loss: 1.5433 - accuracy: 0.3994 - val_loss: 1.5627 - val_accuracy: 0.3831\n",
      "Epoch 18/50\n",
      "449/449 [==============================] - 435s 968ms/step - loss: 1.5217 - accuracy: 0.4092 - val_loss: 1.5159 - val_accuracy: 0.3998\n",
      "Epoch 19/50\n",
      "449/449 [==============================] - 395s 881ms/step - loss: 1.5112 - accuracy: 0.4094 - val_loss: 1.4764 - val_accuracy: 0.4216\n",
      "Epoch 20/50\n",
      "449/449 [==============================] - 348s 775ms/step - loss: 1.4982 - accuracy: 0.4208 - val_loss: 1.4603 - val_accuracy: 0.4344\n",
      "Epoch 21/50\n",
      "449/449 [==============================] - 351s 781ms/step - loss: 1.4790 - accuracy: 0.4297 - val_loss: 1.4474 - val_accuracy: 0.4405\n",
      "Epoch 22/50\n",
      "449/449 [==============================] - 339s 755ms/step - loss: 1.4656 - accuracy: 0.4324 - val_loss: 1.4171 - val_accuracy: 0.4581\n",
      "Epoch 23/50\n",
      "449/449 [==============================] - 336s 749ms/step - loss: 1.4491 - accuracy: 0.4407 - val_loss: 1.4019 - val_accuracy: 0.4642\n",
      "Epoch 24/50\n",
      "449/449 [==============================] - 335s 747ms/step - loss: 1.4354 - accuracy: 0.4424 - val_loss: 1.3872 - val_accuracy: 0.4698\n",
      "Epoch 25/50\n",
      "449/449 [==============================] - 335s 747ms/step - loss: 1.4290 - accuracy: 0.4475 - val_loss: 1.3787 - val_accuracy: 0.4695\n",
      "Epoch 26/50\n",
      "449/449 [==============================] - 336s 749ms/step - loss: 1.4045 - accuracy: 0.4621 - val_loss: 1.3681 - val_accuracy: 0.4675\n",
      "Epoch 27/50\n",
      "449/449 [==============================] - 339s 755ms/step - loss: 1.3910 - accuracy: 0.4652 - val_loss: 1.3783 - val_accuracy: 0.4753\n",
      "Epoch 28/50\n",
      "449/449 [==============================] - 336s 749ms/step - loss: 1.3741 - accuracy: 0.4720 - val_loss: 1.3433 - val_accuracy: 0.4859\n",
      "Epoch 29/50\n",
      "449/449 [==============================] - 335s 747ms/step - loss: 1.3697 - accuracy: 0.4721 - val_loss: 1.3303 - val_accuracy: 0.4937\n",
      "Epoch 30/50\n",
      "449/449 [==============================] - 336s 749ms/step - loss: 1.3461 - accuracy: 0.4794 - val_loss: 1.3098 - val_accuracy: 0.4921\n",
      "Epoch 31/50\n",
      "449/449 [==============================] - 337s 751ms/step - loss: 1.3386 - accuracy: 0.4873 - val_loss: 1.3012 - val_accuracy: 0.4990\n",
      "Epoch 32/50\n",
      "449/449 [==============================] - 336s 749ms/step - loss: 1.3332 - accuracy: 0.4879 - val_loss: 1.3383 - val_accuracy: 0.4834\n",
      "Epoch 33/50\n",
      "449/449 [==============================] - 335s 745ms/step - loss: 1.3185 - accuracy: 0.4945 - val_loss: 1.3055 - val_accuracy: 0.4921\n",
      "Epoch 34/50\n",
      "449/449 [==============================] - 336s 749ms/step - loss: 1.3034 - accuracy: 0.5024 - val_loss: 1.2851 - val_accuracy: 0.5077\n",
      "Epoch 35/50\n",
      "449/449 [==============================] - 335s 746ms/step - loss: 1.2824 - accuracy: 0.5110 - val_loss: 1.2735 - val_accuracy: 0.5065\n",
      "Epoch 36/50\n",
      "449/449 [==============================] - 335s 746ms/step - loss: 1.2771 - accuracy: 0.5090 - val_loss: 1.2771 - val_accuracy: 0.5001\n",
      "Epoch 37/50\n",
      "449/449 [==============================] - 336s 749ms/step - loss: 1.2822 - accuracy: 0.5074 - val_loss: 1.2482 - val_accuracy: 0.5196\n",
      "Epoch 38/50\n",
      "449/449 [==============================] - 376s 837ms/step - loss: 1.2600 - accuracy: 0.5195 - val_loss: 1.2787 - val_accuracy: 0.5038\n",
      "Epoch 39/50\n",
      "449/449 [==============================] - 350s 779ms/step - loss: 1.2495 - accuracy: 0.5252 - val_loss: 1.2360 - val_accuracy: 0.5252\n",
      "Epoch 40/50\n",
      "449/449 [==============================] - 389s 866ms/step - loss: 1.2428 - accuracy: 0.5288 - val_loss: 1.2300 - val_accuracy: 0.5302\n",
      "Epoch 41/50\n",
      "449/449 [==============================] - 354s 788ms/step - loss: 1.2310 - accuracy: 0.5292 - val_loss: 1.2350 - val_accuracy: 0.5247\n",
      "Epoch 42/50\n",
      "449/449 [==============================] - 405s 903ms/step - loss: 1.2269 - accuracy: 0.5333 - val_loss: 1.2268 - val_accuracy: 0.5319\n",
      "Epoch 43/50\n",
      "449/449 [==============================] - 397s 884ms/step - loss: 1.2186 - accuracy: 0.5315 - val_loss: 1.2337 - val_accuracy: 0.5272\n",
      "Epoch 44/50\n",
      "449/449 [==============================] - 350s 779ms/step - loss: 1.2065 - accuracy: 0.5382 - val_loss: 1.2315 - val_accuracy: 0.5305\n",
      "Epoch 45/50\n",
      "449/449 [==============================] - 348s 775ms/step - loss: 1.1990 - accuracy: 0.5442 - val_loss: 1.2150 - val_accuracy: 0.5344\n",
      "Epoch 46/50\n",
      "449/449 [==============================] - 347s 774ms/step - loss: 1.1847 - accuracy: 0.5526 - val_loss: 1.2262 - val_accuracy: 0.5283\n",
      "Epoch 47/50\n",
      "449/449 [==============================] - 351s 782ms/step - loss: 1.1711 - accuracy: 0.5515 - val_loss: 1.1979 - val_accuracy: 0.5430\n",
      "Epoch 48/50\n",
      "449/449 [==============================] - 352s 784ms/step - loss: 1.1782 - accuracy: 0.5520 - val_loss: 1.2011 - val_accuracy: 0.5391\n",
      "Epoch 49/50\n",
      "449/449 [==============================] - 382s 851ms/step - loss: 1.1575 - accuracy: 0.5591 - val_loss: 1.1955 - val_accuracy: 0.5467\n",
      "Epoch 50/50\n",
      "449/449 [==============================] - 513s 1s/step - loss: 1.1673 - accuracy: 0.5542 - val_loss: 1.1932 - val_accuracy: 0.5433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a4ca581970>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrainement du model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=50, verbose=1, validation_data = (X_test, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 15s 137ms/step - loss: 1.1932 - accuracy: 0.5433\n",
      "[1.193182110786438, 0.5433268547058105]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test,y_test)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3) Save model with h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_emotion_detection.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Testing model with camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "import sklearn.externals\n",
    "import joblib\n",
    "import h5py\n",
    "from tkinter import *\n",
    "import tensorflow as tf\n",
    "from PIL import ImageTk, Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Loading the model with H5PY & the face detectcion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model_emotion_detection.h5')\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Program performing emotion detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\romai\\anaconda3\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-5-124a141233fc>\", line 66, in open_webcam\n",
      "    converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
      "cv2.error: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\romai\\anaconda3\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-5-124a141233fc>\", line 66, in open_webcam\n",
      "    converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
      "cv2.error: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\romai\\anaconda3\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-5-124a141233fc>\", line 66, in open_webcam\n",
      "    converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
      "cv2.error: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\romai\\anaconda3\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-5-124a141233fc>\", line 66, in open_webcam\n",
      "    converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
      "cv2.error: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\romai\\anaconda3\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-5-124a141233fc>\", line 66, in open_webcam\n",
      "    converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
      "cv2.error: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\romai\\anaconda3\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-5-124a141233fc>\", line 66, in open_webcam\n",
      "    converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
      "cv2.error: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creer une fenetre\n",
    "window = Tk()\n",
    " \n",
    "# Personnaliser la fenetre\n",
    "window.title(\"Emotion recognition\")\n",
    "window.geometry(\"1200x720\")\n",
    "window.config(background = \"darkblue\")\n",
    " \n",
    "# Premier texte\n",
    "label_title = Label(window, text = \"Present your face in front of the webcam and click on Start button\", font = (\"Arial\", 25), bg = \"darkblue\", fg = \"white\")\n",
    "label_title.pack() #pour afficher le titre\n",
    " \n",
    "# Second texte\n",
    "label_subtitle = Label(window, text = \"Here's your real time emotion : \", font = (\"Arial\", 20), bg = \"darkblue\", fg = \"white\")\n",
    "label_subtitle.pack(expand = YES) #pour afficher le titre\n",
    "\n",
    "# Definition de variables qui vont nous aider pour le programme\n",
    "global activation_sytem\n",
    "activation_sytem = 0 \n",
    "\n",
    "global nbr_iteration\n",
    "nbr_iteration = 0\n",
    "\n",
    "global del_label\n",
    "del_label = 0\n",
    "\n",
    "# Fonction d'activation de la webcam et du code de reconnaissance facial correspondant\n",
    "def open_webcam():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # width, height = 800, 600\n",
    "    # cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    # cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "    activation_sytem = 1\n",
    "    nbr_iteration = 0\n",
    "    predictions_sum = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
    "    \n",
    "    # Variable del_label qui, une fois au dessus de 1, permettra d'éffacer le contenu des Labels de l'interface dans le but de refaire le test de reconnaissance\n",
    "    global del_label\n",
    "    if del_label > 0:\n",
    "        global label_subsubtitle1\n",
    "        label_subsubtitle1.destroy()\n",
    "        global label_subsubtitle2\n",
    "        label_subsubtitle2.destroy()\n",
    "        global label_subsubtitle3\n",
    "        label_subsubtitle3.destroy()\n",
    "        global label_subsubtitle4\n",
    "        label_subsubtitle4.destroy()\n",
    "        global label_subsubtitle5\n",
    "        label_subsubtitle5.destroy()\n",
    "        global label_subsubtitle6\n",
    "        label_subsubtitle6.destroy()\n",
    "        global label_subsubtitle7\n",
    "        label_subsubtitle7.destroy()\n",
    "        global label_subtitle8\n",
    "        label_subtitle8.destroy()\n",
    "        \n",
    "    if del_label == 0:\n",
    "        del_label = 1\n",
    "    \n",
    "    # Partie de code lisant le contenu de la webcam et qui attribue une émotion à ce contenu\n",
    "    while activation_sytem == 1:\n",
    "        \n",
    "        # Lecture de la webcam\n",
    "        ret,image = cap.read()\n",
    "        # convertion de l'image en niveau de gris\n",
    "        converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # Detection du visage sur l'image\n",
    "        faces_detected = face_haar_cascade.detectMultiScale(converted_image,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x,y,w,h) in faces_detected:\n",
    "            \n",
    "            # Dessin du rectangle autour du visage\n",
    "            cv2.rectangle(image,(x,y), (x+w,y+h), (255,0,0))\n",
    "            # Calibration de l'image en 48x48 pixels\n",
    "            roi_gray = converted_image[y:y+w,x:x+h]\n",
    "            roi_gray = cv2.resize(roi_gray,(48,48))\n",
    "            \n",
    "            # Transformation de l'image en un array\n",
    "            image_pixels = tf.keras.preprocessing.image.img_to_array(roi_gray)\n",
    "            image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
    "            image_pixels /= 255\n",
    "            \n",
    "            # Prédiction par le model\n",
    "            predictions = model.predict(image_pixels) \n",
    "            max_index = np.argmax(predictions[0])\n",
    "            \n",
    "            # Attibution de l'émotion (en string) correspondant à la prédiction du model\n",
    "            emotion_detection = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "            emotion_prediction = emotion_detection[max_index]\n",
    "            \n",
    "            #calcul de la somme des probabilités correspondant à chaque émotion\n",
    "            predictions_sum[0][0] = predictions_sum[0][0] + predictions[0][0]\n",
    "            predictions_sum[0][1] = predictions_sum[0][1] + predictions[0][1]\n",
    "            predictions_sum[0][2] = predictions_sum[0][2] + predictions[0][2]\n",
    "            predictions_sum[0][3] = predictions_sum[0][3] + predictions[0][3]\n",
    "            predictions_sum[0][4] = predictions_sum[0][4] + predictions[0][4]\n",
    "            predictions_sum[0][5] = predictions_sum[0][5] + predictions[0][5]\n",
    "            predictions_sum[0][6] = predictions_sum[0][6] + predictions[0][6]\n",
    "            \n",
    "            # Incrémentation de la variable pour connaître le nombre d'itération effectué et ensuite pouvoir faire une moyenne des émotions\n",
    "            nbr_iteration = nbr_iteration + 1\n",
    "            \n",
    "            # Détermination de la police d'écriture\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            # Emplacement et propriétés du text \"Emotion\" placé sur l'image\n",
    "            cv2.putText(image, emotion_prediction, (int(x), int(y)), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
    "            \n",
    "            #cv.NamedWindow(window, cv.CV_WINDOW_NORMAL)\n",
    "            #root = Tk()\n",
    "            #root.geometry(\"1200x720\")\n",
    "            #root.config(background = \"black\")\n",
    "            #Label(root,text=\"Rom\", font=(\"times new roman\",30,\"bold\"), bg=\"black\", fg=\"red\").pack()\n",
    "            #f1 = LabelFrame (root, bg=\"red\")\n",
    "            #f1.pack()\n",
    "            #L1 = Label (f1, bg=\"red\")\n",
    "            #L1.pack()\n",
    "            #img = ImageTk.PhotoImage(Image.fromarray(image))\n",
    "            #L1['image'] = img\n",
    "            \n",
    "            #root.update()\n",
    "            #cv2.imshow('Demo', image)\n",
    "            # Create a frame\n",
    "            #app = Frame(window, bg=\"white\")\n",
    "            \n",
    "            \n",
    "            # Create a label in the frame\n",
    "            #main = Label(app)\n",
    "            #lmain.pack(expand = YES)\n",
    "            \n",
    "            #img = Image.fromarray(image)\n",
    "            #imgtk = ImageTk.PhotoImage(image=img)\n",
    "            #lmain.imgtk = imgtk\n",
    "            #lmain.configure(image=imgtk)\n",
    "            #app.pack(expand = YES)\n",
    "            \n",
    "           \n",
    "            \n",
    "            #image.canvas = Canvas(window, width=200, height=200, bg = \"darkblue\")\n",
    "            #canvas.pack(expand = YES)\n",
    "           \n",
    "            \n",
    "            # Affichage de l'image/vidéo\n",
    "            cv2.imshow('Demo video', image)\n",
    "    \n",
    "           \n",
    "            # Arrêt de la démonstration une fois avoir atteint le nombre de 60 itérations\n",
    "            if cv2.waitKey(2) & nbr_iteration == 60:\n",
    "                \n",
    "                # Arret de l'utilisation de la webcam\n",
    "                cap.release()\n",
    "                \n",
    "                # calcul de la moyenne des prédictions de chaque émotions sur l'entierté de la démonstration\n",
    "                moyenne_angry = (predictions_sum[0][0]/nbr_iteration)*100\n",
    "                mo_angry = round(moyenne_angry,1)\n",
    "                \n",
    "                moyenne_disgust = (predictions_sum[0][1]/nbr_iteration)*100\n",
    "                mo_disgust = round(moyenne_disgust,1)\n",
    "                \n",
    "                moyenne_fear = (predictions_sum[0][2]/nbr_iteration)*100\n",
    "                mo_fear = round(moyenne_fear,1)\n",
    "                \n",
    "                moyenne_happy = (predictions_sum[0][3]/nbr_iteration)*100\n",
    "                mo_happy = round(moyenne_happy,1)\n",
    "                \n",
    "                moyenne_sad = (predictions_sum[0][4]/nbr_iteration)*100\n",
    "                mo_sad = round(moyenne_sad,1)\n",
    "                \n",
    "                moyenne_surprise = (predictions_sum[0][5]/nbr_iteration)*100\n",
    "                mo_surprise = round(moyenne_surprise,1)\n",
    "                \n",
    "                moyenne_neutral = (predictions_sum[0][6]/nbr_iteration)*100\n",
    "                mo_neutral = round(moyenne_neutral,1)\n",
    "                \n",
    "                #Determination de l'émotion dominante\n",
    "                tab_moyenne = [[mo_angry, mo_disgust, mo_fear, mo_happy, mo_sad, mo_surprise, mo_neutral]]\n",
    "                max_moy = np.argmax(tab_moyenne[0])\n",
    "                dominant = emotion_detection[max_moy]\n",
    "                \n",
    "                #Affichage de ces moyennes sur l'interface\n",
    "                label_subsubtitle1 = Label(window, text = ('angry', mo_angry,'%') , font = (\"Arial\", 20), bg = \"darkblue\", fg = \"white\")\n",
    "                label_subsubtitle2 = Label(window, text = ('disgust', mo_disgust,'%') , font = (\"Arial\", 20), bg = \"darkblue\", fg = \"white\")\n",
    "                label_subsubtitle3 = Label(window, text = ('fear', mo_fear,'%') , font = (\"Arial\", 20), bg = \"darkblue\", fg = \"white\")\n",
    "                label_subsubtitle4 = Label(window, text = ('happy', mo_happy,'%') , font = (\"Arial\", 20), bg = \"darkblue\", fg = \"white\")\n",
    "                label_subsubtitle5 = Label(window, text = ('sad', mo_sad,'%') , font = (\"Arial\", 20), bg = \"darkblue\", fg = \"white\")\n",
    "                label_subsubtitle6 = Label(window, text = ('surprise', mo_surprise,'%') , font = (\"Arial\", 20), bg = \"darkblue\", fg = \"white\")\n",
    "                label_subsubtitle7 = Label(window, text = ('neutral', mo_neutral,'%') , font = (\"Arial\", 20), bg = \"darkblue\", fg = \"white\")\n",
    "                \n",
    "                label_subsubtitle1.pack(expand = YES) \n",
    "                label_subsubtitle2.pack(expand = YES) \n",
    "                label_subsubtitle3.pack(expand = YES)\n",
    "                label_subsubtitle4.pack(expand = YES)\n",
    "                label_subsubtitle5.pack(expand = YES)\n",
    "                label_subsubtitle6.pack(expand = YES)\n",
    "                label_subsubtitle7.pack(expand = YES)\n",
    "                \n",
    "                #Affichage de l'émotion dominante sur l'interface\n",
    "                label_subtitle8 = Label(window, text = ('Dominant:', dominant) , font = (\"Arial\", 30), bg = \"darkblue\", fg = \"red\")\n",
    "                label_subtitle8.pack(expand = YES) #pour afficher le titre\n",
    "                \n",
    "                # Fermeture de la fenêtre contenant la webcam\n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "                # Arrêt de la boucle for\n",
    "                break\n",
    "               \n",
    "# Fonction Stop permettant la fermeture de la fenêtre d'interface\n",
    "def stop():\n",
    "    activation_sytem=0\n",
    "    window.destroy()\n",
    "        \n",
    "#  Ajout des boutons Start et Close permettant donc de lancer la démonstration et de fermer la fenêtre d'interface\n",
    "button_Stop = Button(window, text = \"Close\", font =(\"Arial\", 20), bg = 'white', fg = 'darkblue', command = stop)\n",
    "button_Stop.pack(side = BOTTOM, fill = X, expand = False)\n",
    "\n",
    "button_Start = Button(window, text = \"Start\", font =(\"Arial\", 20), bg = 'white', fg = 'darkblue', command = open_webcam)\n",
    "button_Start.pack(side = BOTTOM, fill = X, expand = False)\n",
    "\n",
    "# Affichage de l'interface\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source :\n",
    "- https://analyticsindiamag.com/my-first-cnn-project-emotion-detection-using-convolutional-neural-network-with-tpu/\n",
    "\n",
    "- https://www.youtube.com/watch?v=DtBu1u5aBsc&t=2363s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
